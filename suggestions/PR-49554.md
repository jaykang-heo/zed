# PR #49554: Allow using Zeta through an arbitrary OpenAI-compatible self-hosted API

_Merged: 2026-02-19T20:21:04Z_
_PR: https://github.com/zed-industries/zed/pull/49554_


## Documentation Suggestions

### Summary
This change adds support for arbitrary OpenAI-compatible APIs as an edit prediction provider, extends the existing Ollama configuration with a `prompt_format` setting, and refactors the codebase to use a unified FIM (Fill-In-the-Middle) approach for both Ollama and the new OpenAI-compatible provider. Users can now configure custom self-hosted edit prediction servers.

### Suggested Changes

#### 1. docs/src/ai/edit-prediction.md

- **Section**: Multiple sections need updates (provider list, configuration examples, and new OpenAI-compatible section)
- **Change**: Add/Update
- **Target keyword**: self-hosted code completion Zed
- **Frontmatter**:
  ```yaml
  ---
  title: Edit Prediction in Zed
  description: Configure AI-powered code completion in Zed with cloud providers, Ollama, or custom OpenAI-compatible servers for real-time coding assistance.
  ---
  ```
- **Links**: 
  - [Language Settings](/docs/configuring-languages)
  - [Ollama extension documentation](/extensions/ollama) 
  - [Zed AI features overview](https://zed.dev/ai) (marketing link)
  - [Settings reference](/docs/settings)
- **Full-file brand pass**: Required: yes. After reviewing the existing file, the provider descriptions and setup instructions appear to follow the brand voice. The new sections below integrate naturally and maintain technical grounding, direct tone, and developer respect throughout.

- **Suggestion**:

**In the providers list section**, add after the Ollama entry:

> **Preview:** This feature is available in Zed Preview. It will be included in the next Stable release.

```markdown
**OpenAI-Compatible API**: Use any OpenAI-compatible self-hosted server (vLLM, llama.cpp server, LocalAI, etc.) for edit predictions with customizable prompt formats.
```

**Add a new section after the Ollama configuration section**:

```markdown
### OpenAI-Compatible API

You can use any self-hosted server that implements the OpenAI completion API format. This works with vLLM, llama.cpp server, LocalAI, and other compatible servers.

> **Preview:** This feature is available in Zed Preview. It will be included in the next Stable release.

#### Configuration

Set `open_ai_compatible_api` as your provider and configure the API endpoint:

```json
{
  "edit_predictions": {
    "provider": "open_ai_compatible_api",
    "open_ai_compatible_api": {
      "api_url": "http://localhost:8080/v1/completions",
      "model": "deepseek-coder-6.7b-base",
      "prompt_format": "deepseek_coder",
      "max_output_tokens": 64
    }
  }
}
```

The `prompt_format` setting controls how code context is formatted for the model. Use `"infer"` to detect the format from the model name, or specify one explicitly:

- `code_llama` - CodeLlama format: `<PRE> prefix <SUF> suffix <MID>`
- `star_coder` - StarCoder format: `<fim_prefix>prefix<fim_suffix>suffix<fim_middle>`
- `deepseek_coder` - DeepSeek format with special unicode markers
- `qwen` - Qwen/CodeGemma format: `<|fim_prefix|>prefix<|fim_suffix|>suffix<|fim_middle|>`
- `codestral` - Codestral format: `[SUFFIX]suffix[PREFIX]prefix`
- `glm` - GLM-4 format with code markers
- `infer` - Auto-detect from model name (default)

Your server must implement the OpenAI `/v1/completions` endpoint. Edit predictions will send POST requests with this format:

```json
{
  "model": "your-model-name",
  "prompt": "formatted-code-context",
  "max_tokens": 256,
  "temperature": 0.2,
  "stop": ["<|endoftext|>", ...]
}
```
```

**Update the Ollama section** to mention the new `prompt_format` setting:

> **Changed in Preview (v0.225).** See [release notes](/releases#0.225).

Find the existing Ollama configuration example and update it to:

```json
{
  "edit_predictions": {
    "provider": "ollama",
    "ollama": {
      "api_url": "http://localhost:11434",
      "model": "qwen2.5-coder:7b-base",
      "prompt_format": "infer",
      "max_output_tokens": 64
    }
  }
}
```

Add after the configuration example:

```markdown
The `prompt_format` setting works the same as for OpenAI-compatible APIs (see above). When set to `"infer"`, Zed detects the format from your model name. Explicitly specify a format if auto-detection doesn't match your model.
```

- **Brand voice scorecard**:

  | Criterion            | Score | Notes |
  | -------------------- | ----- | ----- |
  | Technical Grounding  | 5     | Specifies exact API format, config fields, supported servers |
  | Natural Syntax       | 4     | Direct second-person, present tense throughout |
  | Quiet Confidence     | 5     | States capabilities without hedging or hype |
  | Developer Respect    | 5     | Assumes technical competence, provides complete info |
  | Information Priority | 5     | Config example first, then explanatory details |
  | Specificity          | 5     | Exact JSON schemas, endpoint paths, format options |
  | Voice Consistency    | 4     | Matches existing edit-prediction.md tone |
  | Earned Claims        | 5     | Only claims what the code implements |
  | **TOTAL**            | 38/40 |       |

  Pass threshold: all criteria 4+ âœ“

### Notes for Reviewer

1. **Preview callouts**: The new OpenAI-compatible API feature is entirely new, so it uses the "Preview" callout. The Ollama `prompt_format` setting modifies existing behavior, so it uses "Changed in Preview" with a version reference (update version number to match actual release).

2. **Prompt format values**: The code supports additional formats beyond those listed (e.g., `code_gemma` as a separate enum variant), but I consolidated similar formats in the docs (Qwen/CodeGemma use the same markers). The full list in `EditPredictionPromptFormat` includes: `Infer`, `CodeLlama`, `StarCoder`, `DeepseekCoder`, `Qwen`, `CodeGemma`, `Codestral`, `Glm`. Consider whether to list all variants or keep the consolidated set.

3. **API endpoint clarity**: The code refactoring creates a unified FIM approach but doesn't expose Zeta model support through custom servers (that still requires cloud credentials). The documentation focuses on FIM-based models, which is the primary use case for self-hosted scenarios.

4. **Settings Editor UI**: This change is settings-based and doesn't have a corresponding Settings Editor UI panel (based on the code changes). The JSON-first approach is appropriate for power users configuring custom servers.
